

############################################################################################
from pyspark.sql.functions import row_number, monotonically_increasing_id,col
from pyspark.sql import Window
from pyspark.sql import functions as F


group_by_cols=["Job_scheduled"]

df=spark.sql("select * from delta.`/mnt/gca-assets-lakehouse/marts/schedule_info`")\
.withColumn("index",row_number().over(Window.orderBy(monotonically_increasing_id())))\
.withColumn('ct',F.count("*").over(Window.partitionBy(group_by_cols)))\
.withColumn('SumIndex',F.sum('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MaxIndex',F.max('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MinIndex',F.min('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('AverageIndex',F.avg('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('ct_all',F.count("*").over(Window.partitionBy()))\
.withColumn("pct", col("ct")/col("ct_all"))
############################################################################################
