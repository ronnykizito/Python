rom datetime import datetime,date,timedelta
from pandas.tseries import offsets
from dateutil.relativedelta import relativedelta
from dateutil.rrule import rrule, MONTHLY,DAILY,YEARLY
start_date =  date.today() - offsets.YearBegin() - relativedelta(years=1)
end_date = date.today() #+ offsets.YearEnd()
periods = [dt.strftime("%Y%m") for dt in rrule(MONTHLY, dtstart=start_date,until=end_date+relativedelta(months=0))]

for col_name in df.columns: df = df.withColumn(col_name, df[col_name].cast(StringType()))
for col_name in cast_as_DoubleType: df = df.withColumn(col_name, df[col_name].cast(DoubleType()))
for col_name in cast_as_IntegerTyp: df = df.withColumn(col_name, df[col_name].cast(IntegerType()))
for col_name in cast_as_TimestampType: df = df.withColumn(col_name, df[col_name].cast(TimestampType()))
for col_name in cast_as_DateType: df = df.withColumn(col_name, df[col_name].cast(DateType()))

split(F.element_at(F.split(F.input_file_name(), "/"),-1), '.csv').getItem(0).alias("file_name")

#https://docs.microsoft.com/en-us/azure/open-datasets/dataset-us-population-zip?tabs=azureml-opendatasets

tuple(list(df.toPandas()['Organisationsnamn'].unique()))

list(map(lambda x: x.upper(), test))


gender_wear=["Menswear","Womenswear"]

from multiprocessing.pool import ThreadPool
parallels=ThreadPool(2)


parallels.map(export_segments_to_sas, gender_wear)







from multiprocessing.pool import ThreadPool

dbutils.widgets.text("widget_name","")
my_var=dbutils.widgets.get("widget_name")

notebooks=["/Repos/gca-projects/azure-sas-prod/Analytic_Env/02_segments/04_cos_menswear_segments" $widget_name="test",
           "/Repos/gca-projects/azure-sas-prod/Analytic_Env/02_segments/03_cos_womenswear_segments"]
pool = ThreadPool(len(notebooks))
pool.map(lambda path: dbutils.notebook.run(path, timeout_seconds= 6000, arguments={"input-data": path}),notebooks)

dbutils.notebook.run("/Repos/rapporter@pbm.se/PBM_REPORTS_PROD/test1", 60, {"widget_name": "my-data"})





multi argument

from multiprocessing.dummy import Pool as ThreadPool
import itertools

pool = ThreadPool(16)
pool.starmap(get_data_from_vismanet_glt, itertools.product(periods,CompanyIds))
pool.close()
pool.join() 
############################################################################################

from pyspark.sql.functions import *
from pyspark.sql import Window,functions as F




group_by_cols=["Job_scheduled"]

df=spark.sql("select * from delta.`/mnt/gca-assets-lakehouse/marts/schedule_info`")\
.withColumn("index",row_number().over(Window.orderBy(monotonically_increasing_id())))\
.dropDuplicates(['country'])\
.withColumn("index",monotonically_increasing_id()+1)\
.withColumn("cum_",row_number().over(Window.partitionBy(group_by_cols).orderBy("kontonivå3")))\
.withColumn("hash_key",sha2(col("kontonivå3").cast("string"),512))\
.withColumn('ct',F.count("*").over(Window.partitionBy(group_by_cols)))\
.withColumn('SumIndex',F.sum('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MaxIndex',F.max('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MinIndex',F.min('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('AverageIndex',F.avg('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('ct_all',F.count("*").over(Window.partitionBy()))\
.withColumn("pct", col("ct")/col("ct_all"))\
.withColumn("cum_",row_number().over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lag",lag("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lead",lead("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("pct1", concat("Destination",lit("."),"Updated_file"))\
.withColumn("one", lit(1))\
.withColumn("substr", substring("Destination",1,3))\
.withColumn("substr1",  col("Destination").substr(2,4))\
.withColumn('split', split(col('Destination'), '_').getItem(0))\
.withColumn('part_date', date_format('Last_updated_time_in_Azure',"MMMM"))\
.withColumn("Last_updated_time_in_Azure",from_utc_timestamp(current_timestamp(),'Europe/Stockholm'))\
.sort(['Value','ID'], ascending=[False,True])
.withColumn('dag', substring("CLOSED_DATE",1,2))\
.withColumn('man', lower(substring("CLOSED_DATE",3,3)))\
.withColumn('ar', substring("CLOSED_DATE",6,4))\
.withColumn('monthNumber', date_format(to_date(col('man'), 'MMM'), 'MM'))\


casewhen="""
case 
when man ="JAN" then '01' 
when man ="FEB" then '02'
when man ="MAR" then '03'
when man ="APR" then '04'
when man ="MAY" then '05'
when man ="JUN" then '06'
when man ="JUL" then '07'
when man ="AUG" then '08'
when man ="SEP" then '09'
when man ="OCT" then '10'
when man ="NOV" then '11'
when man ="DEC" then '12'

end 
"""

for month_col in month_cols:
    df=df.withColumn("dag", substring(month_col,1,2))\
    .withColumn('dag', lpad("dag",2, '0'))\
    .withColumn("man", upper(substring(month_col,4,3)))\
    .withColumn("ar", substring(month_col,8,4))\
    .withColumn("monthnr", expr(casewhen))\
    .withColumn(month_col, concat("ar",lit("-"),"monthnr",lit("-"),"dag"))\
    .drop('dag','man','ar','monthnr')

for col_name in df.columns:df = df.withColumn(col_name, df[col_name].cast(StringType()))

df = spark.createDataFrame(data).sort(['Value'], ascending=[False]).drop_duplicates(['Category','ID'])\


df = spark.createDataFrame([("mytest", 34)], ["createdby", "age"]).drop('age')\
.withColumn("updated",from_utc_timestamp(current_timestamp(),'Europe/Stockholm'))

casewhen="""
case when passengerCount =1 then 'one' 
when passengerCount between 2 and 3 then '2and3' else 'test' end 
"""

df = nyc_tlc.to_spark_dataframe()\
.withColumn("casewhere", expr(casewhen))\

from pyspark.sql import functions as F

df = spark.read.format('csv')\
.option("header",True)\
.option("delimiter",';')\
.load('/mnt/dev/source-systems/test/*.csv').withColumn("filename", F.input_file_name())
############################################################################################

pivot_hold_columns = ['Period','Type','Account','Cost Center','Market','Business Unit','Product']
df = ps.melt(df, id_vars=pivot_hold_columns, var_name="Objecttypbeskrivning", value_name="Budget")

readcsv
from pyspark.sql.functions import *
from pyspark.sql import Window,functions as F
from pyspark.sql.types import *

df = spark.read.format("csv")\
.option("header",True)\
.option("inferSchema",False)\
.option("delimiter",",")\
.option("treatEmptyValuesAsNulls", False)\
.option("encoding", "iso-8859-1")\
.load("/mnt/source-systems/customer-value/from-customer-value/csv")\
.withColumn("filename", F.input_file_name())\
.withColumn('last_updated', substring("cve_last_updated",1,10).cast(TimestampType()))\
.drop("cve_last_updated")\
.withColumn('split_l', split(col('profit_online_value_segment'), '_').getItem(0))\
.withColumn('split_r', split(col('profit_online_value_segment'), '_').getItem(1))\
.withColumn("concat", concat("country",lit("."),"profit_online_frequency_segment","split_r"))\
.withColumn("online_profit_12m", col("online_profit_12m").cast(DoubleType()))\
.withColumn("online_return_rate_12m", col("online_return_rate_12m").cast(DoubleType()))\
.withColumn("online_frequency_12m", col("online_frequency_12m").cast(IntegerType()))\
.withColumn("casewhen", expr("case when online_frequency_12m between 2 and 7 then '2-7' else 'else' end"))
.withColumn('v1', regexp_replace('v1', 'a', 'AAA'))


for col_name in df.columns: df = df.withColumn(col_name, df[col_name].cast(StringType())) # Assign String type to all variables of a table
for col_name in cast_as_integer: df = df.withColumn(col_name, df[col_name].cast(IntegerType())) # Assign Integer type to numeric variables
for col_name in cast_as_decimal: df = df.withColumn(col_name, df[col_name].cast(DoubleType())) # Assign Dobule type to decimal variables
for col_name in cast_as_timestamp: df = df.withColumn(col_name, df[col_name].cast(TimestampType())) # Assign Datetime type to Date or Datetime variables

df=df.toDF(*(c.replace('.', '_') for c in df.columns))
  df=df.select([F.col(col).alias(col.lower()) for col in df.columns])\

df=df.na.fill(value=0) #Replace 0 for null for all integer columns. Otherwise PowerBI can define them as text 
df=df.na.fill("") #Replace null with empty for all text columns

display(df)

##########################################################################################
df1=df.withColumn('ct',F.count("*").over(Window.partitionBy('country')))\
.withColumn('ct_all',F.count("*").over(Window.partitionBy()))\
.withColumn("pct", col("ct")/col("ct_all"))\
.dropDuplicates(['country'])\
.withColumn('ct_country',F.count("*").over(Window.partitionBy()))\
.sort("country")\
.withColumn('SumIndex',F.sum('pct').over(Window.partitionBy()))\
.select('country','ct','ct_all','pct')

%sql

with q1 as (
select country,count(*) as ct, 1 as id from csv group by 1),
q2 as (select count(*) as ct_all, 1 as id,count(DISTINCT country) as ct_country from csv)

select q1.*,q2.*, ct/ct_all as ct_pct from q1 join q2 on q1.id=q2.id order by 1

########################################################################################################

#read_sql_query=f'''
#CREATE database IF NOT EXISTS {database} COMMENT 'This is reporting database fro longrunning jobs' LOCATION "{lakehouse_path}"
#'''

#spark.sql(read_sql_query)

spark.sql(f"drop table if exists {business_assets_db}.{test}")
spark.sql(f"create table {business_assets_db}.{test} using delta location '{delta_path}' ")

#spark.sql("DROP TABLE if exists " + database+'.'+table_name)
#spark.sql("CREATE TABLE  " + database+'.'+table_name + " USING DELTA LOCATION '" + pathToMart + "'")

from delta.tables import *
# Vaccum (100) means in following command that remove all files older then 100 days from lakehouse.

#deltaTable = DeltaTable.forPath(spark, pathToTable)
#deltaTable.vacuum(100)
#optimize_table='''OPTIMIZE delta.`%s`'''%(pathToTable)
#spark.sql(optimize_table)


from pyspark import pandas as ps
from geopy.geocoders import Nominatim
geolocator = Nominatim(user_agent="geoapiExercises")
def city_state_country(row):
    coord = f"{row['LATITUDE']}, {row['LONGITUDE']}"
    location = geolocator.reverse(coord, exactly_one=True)
    address = location.raw['address']
    city = address.get('city', '')
    state = address.get('state', '')
    country = address.get('country', '')
    row['city'] = city
    row['state'] = state
    row['country'] = country
    return row

df = (df.pandas_api().apply(city_state_country, axis=1)).to_spark()
display(df)

json normalization

def create_delta_files(period,CompanyId):
  global df
  getdata_vismanet(period,CompanyId)
  df =spark.createDataFrame( pd.json_normalize(finale_list))
  
  df=df.withColumn("CompanyId", lit(CompanyId))\
  .withColumn("organisation_name", expr("case when CompanyId=2520375 then 'Bemce' \
  when CompanyId=2520362 then 'PBM'  when CompanyId=2680244 then 'PPÖ' else 'Error' end"))\
  .withColumn("updated",from_utc_timestamp(current_timestamp(),'Europe/Stockholm'))\
  .withColumn("selected_year", substring("period",1,4))\
  .withColumn("selected_period",  lit(period))

