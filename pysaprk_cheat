

from multiprocessing.pool import ThreadPool
parallels=ThreadPool(8)
parallels.map(UDF, [company for company in companys])

############################################################################################

from pyspark.sql.functions import *
from pyspark.sql import Window,functions as F




group_by_cols=["Job_scheduled"]

df=spark.sql("select * from delta.`/mnt/gca-assets-lakehouse/marts/schedule_info`")\
.withColumn("index",row_number().over(Window.orderBy(monotonically_increasing_id())))\
.withColumn('ct',F.count("*").over(Window.partitionBy(group_by_cols)))\
.withColumn('SumIndex',F.sum('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MaxIndex',F.max('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MinIndex',F.min('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('AverageIndex',F.avg('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('ct_all',F.count("*").over(Window.partitionBy()))\
.withColumn("pct", col("ct")/col("ct_all"))\
.withColumn("cum_",row_number().over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lag",lag("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lead",lead("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("pct1", concat("Destination",lit("."),"Updated_file"))\
.withColumn("one", lit(1))\
.withColumn("substr", substring("Destination",1,3))\
.withColumn("substr1",  col("Destination").substr(2,4))\
.withColumn('split', split(col('Destination'), '_').getItem(0))\
.withColumn('part_date', date_format('Last_updated_time_in_Azure',"MMMM"))\
.withColumn("Last_updated_time_in_Azure",from_utc_timestamp(current_timestamp(),'Europe/Stockholm'))\

df.display()

df.display()
############################################################################################
