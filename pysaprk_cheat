rom datetime import datetime,date,timedelta
from pandas.tseries import offsets
from dateutil.relativedelta import relativedelta
from dateutil.rrule import rrule, MONTHLY,DAILY,YEARLY
start_date =  date.today() - offsets.YearBegin() - relativedelta(years=1)
end_date = date.today() #+ offsets.YearEnd()
periods = [dt.strftime("%Y%m") for dt in rrule(MONTHLY, dtstart=start_date,until=end_date+relativedelta(months=0))]



#https://docs.microsoft.com/en-us/azure/open-datasets/dataset-us-population-zip?tabs=azureml-opendatasets

tuple(list(df.toPandas()['Organisationsnamn'].unique()))

from multiprocessing.pool import ThreadPool
parallels=ThreadPool(8)
parallels.map(UDF, [company for company in companys])



from multiprocessing.pool import ThreadPool

notebooks=["/Repos/gca-projects/azure-sas-prod/Analytic_Env/02_segments/04_cos_menswear_segments",
           "/Repos/gca-projects/azure-sas-prod/Analytic_Env/02_segments/03_cos_womenswear_segments"]
pool = ThreadPool(len(notebooks))
pool.map(lambda path: dbutils.notebook.run(path, timeout_seconds= 6000, arguments={"input-data": path}),notebooks)



multi argument

from multiprocessing.dummy import Pool as ThreadPool
import itertools

pool = ThreadPool(16)
pool.starmap(get_data_from_vismanet_glt, itertools.product(periods,CompanyIds))
pool.close()
pool.join() 
############################################################################################

from pyspark.sql.functions import *
from pyspark.sql import Window,functions as F




group_by_cols=["Job_scheduled"]

df=spark.sql("select * from delta.`/mnt/gca-assets-lakehouse/marts/schedule_info`")\
.withColumn("index",row_number().over(Window.orderBy(monotonically_increasing_id())))\
.withColumn('ct',F.count("*").over(Window.partitionBy(group_by_cols)))\
.withColumn('SumIndex',F.sum('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MaxIndex',F.max('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('MinIndex',F.min('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('AverageIndex',F.avg('index').over(Window.partitionBy(group_by_cols)))\
.withColumn('ct_all',F.count("*").over(Window.partitionBy()))\
.withColumn("pct", col("ct")/col("ct_all"))\
.withColumn("cum_",row_number().over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lag",lag("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("lead",lead("cum_",2).over(Window.partitionBy(group_by_cols).orderBy("index")))\
.withColumn("pct1", concat("Destination",lit("."),"Updated_file"))\
.withColumn("one", lit(1))\
.withColumn("substr", substring("Destination",1,3))\
.withColumn("substr1",  col("Destination").substr(2,4))\
.withColumn('split', split(col('Destination'), '_').getItem(0))\
.withColumn('part_date', date_format('Last_updated_time_in_Azure',"MMMM"))\
.withColumn("Last_updated_time_in_Azure",from_utc_timestamp(current_timestamp(),'Europe/Stockholm'))\


casewhen="""
case when passengerCount =1 then 'one' 
when passengerCount between 2 and 3 then '2and3' else 'test' end 
"""

df = nyc_tlc.to_spark_dataframe()\
.withColumn("casewhere", expr(casewhen))\

from pyspark.sql import functions as F

df = spark.read.format('csv')\
.option("header",True)\
.option("delimiter",';')\
.load('/mnt/dev/source-systems/test/*.csv').withColumn("filename", F.input_file_name())
############################################################################################

pivot_columns = ['Period','Type','Account','Cost Center','Market','Business Unit','Product']
df = ps.melt(df, id_vars=pivot_columns, var_name="Objecttypbeskrivning", value_name="Budget")
